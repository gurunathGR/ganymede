{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# The magic commands below allow reflecting the changes in an imported module without restarting the kernel.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# We need to add balsam and the modules it depends on to the Python search paths. \n",
    "import sys\n",
    "sys.path.insert(0,'/soft/datascience/Balsam/0.3.5.1/env/lib/python3.6/site-packages/')\n",
    "sys.path.insert(0,'/soft/datascience/Balsam/0.3.5.1/')\n",
    "\n",
    "# We also need postgresql to be in the path\n",
    "import os\n",
    "os.environ['PATH'] ='/soft/datascience/Balsam/0.3.5.1/env/bin/:' + os.environ['PATH']\n",
    "os.environ['PATH'] +=':/soft/datascience/PostgreSQL/9.6.12/bin/'\n",
    "\n",
    "try:\n",
    "    import balsam\n",
    "except:\n",
    "    print('Cannot find balsam, make sure balsam is installed or it is available in Python search paths')    \n",
    "\n",
    "# We also need to activate Balsam database by setting the BALSAM_DB_PATH environment variable. \n",
    "# This is equivalent to `source balsamactivate jupyter_test` \n",
    "os.environ[\"BALSAM_DB_PATH\"]='/lus/theta-fs0/projects/connectomics_aesp/balsam_database/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_database_paths(verbose=True):\n",
    "    \"\"\"\n",
    "    Prints the paths for existing balsam databases\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from balsam.django_config.db_index import refresh_db_index\n",
    "        databasepaths = refresh_db_index()\n",
    "    except:\n",
    "        databasepaths = None\n",
    "    if verbose:\n",
    "        if len(databasepaths) > 0:\n",
    "            print(f'Found {len(databasepaths)} balsam database locations')\n",
    "            for db in databasepaths:\n",
    "                print(db)\n",
    "        else:\n",
    "            print('No balsam database found')\n",
    "    return databasepaths\n",
    "\n",
    "def get_active_database(verbose=True):\n",
    "    \"\"\"\n",
    "    Gets the activate database set in environment variable BALSAM_DB_PATH\n",
    "    Parameters:\n",
    "    verbose: Boolean, (True): Prints verbose info (False): No print\n",
    "    Returns\n",
    "    -------\n",
    "    str, path for the active database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        db = os.environ[\"BALSAM_DB_PATH\"]\n",
    "        if verbose: print(f'Active balsam database path: {db}')\n",
    "    except:\n",
    "        if verbose: print('BALSAM_DB_PATH is not set')\n",
    "        db = None\n",
    "    return db\n",
    "    \n",
    "def add_app(name, executable, description='', envscript='', preprocess='', postprocess='', checkexe=False):\n",
    "    \"\"\"\n",
    "    Adds a new app to the balsam database.\n",
    "    \"\"\"\n",
    "    from balsam.core.models import ApplicationDefinition as App\n",
    "    import shutil\n",
    "    newapp = App()\n",
    "    if checkexe:\n",
    "        if shutil.which(executable):        \n",
    "            print('{} is found'.format(executable))\n",
    "        else:\n",
    "            print('{} is not found'.format(executable))\n",
    "            return newapp\n",
    "        \n",
    "    if App.objects.filter(name=name).exists():\n",
    "        print(\"An application named {} already exists\".format(name))\n",
    "    else:\n",
    "        newapp.name        = name\n",
    "        newapp.executable  = executable\n",
    "        newapp.description = description\n",
    "        newapp.envscript   = envscript\n",
    "        newapp.preprocess  = preprocess\n",
    "        newapp.postprocess = postprocess\n",
    "        newapp.save()\n",
    "    return newapp\n",
    "\n",
    "def get_apps(verbose=True):\n",
    "    \"\"\"\n",
    "    Returns all apps as a list\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from balsam.core.models import ApplicationDefinition as App\n",
    "        apps = App.objects.all()\n",
    "    except:\n",
    "        apps = None\n",
    "    return apps\n",
    "        \n",
    "def get_job():\n",
    "    from balsam.launcher.dag import BalsamJob\n",
    "    return BalsamJob()\n",
    "\n",
    "def add_job(name, workflow, application, description='', args='', num_nodes=1, ranks_per_node=1,cpu_affinity='depth',data={},environ_vars={}):\n",
    "    from balsam.launcher.dag import BalsamJob\n",
    "    job                = BalsamJob()\n",
    "    job.name           = name\n",
    "    job.workflow       = workflow\n",
    "    job.application    = application\n",
    "    job.description    = description\n",
    "    job.args           = args\n",
    "    job.num_nodes      = num_nodes\n",
    "    job.ranks_per_node = ranks_per_node\n",
    "    job.cpu_affinity   = cpu_affinity\n",
    "    job.environ_vars   = environ_vars\n",
    "    job.data           = {}\n",
    "    job.save()\n",
    "    \n",
    "def submit(project='datascience',queue='debug-flat-quad',nodes=1,wall_minutes=30,job_mode='mpi',wf_filter=''):\n",
    "    \"\"\"\n",
    "    Submits a job to the queue with the given parameters.\n",
    "    Parameters\n",
    "    ----------\n",
    "    project: str, name of the project to be charged\n",
    "    queue: str, queue name, can be: 'default', 'debug-cache-quad', or 'debug-flat-quad'\n",
    "    nodes: int, Number of nodes, can be any integer from 1 to 4096.\n",
    "    wall_minutes: int, max wall time in minutes\n",
    "    job_mode: str, Balsam job mode, can be 'mpi', 'serial'\n",
    "    wf_filter: str, Selects Balsam jobs that matches the given workflow filter.\n",
    "    \"\"\"\n",
    "    from balsam import setup\n",
    "    setup()\n",
    "    from balsam.service import service\n",
    "    from balsam.core import models\n",
    "    QueuedLaunch = models.QueuedLaunch\n",
    "    mylaunch = QueuedLaunch()\n",
    "    mylaunch.project = project\n",
    "    mylaunch.queue = queue\n",
    "    mylaunch.nodes = nodes\n",
    "    mylaunch.wall_minutes = wall_minutes\n",
    "    mylaunch.job_mode = job_mode\n",
    "    mylaunch.wf_filter = wf_filter\n",
    "    mylaunch.prescheduled_only=False\n",
    "    mylaunch.save()\n",
    "    service.submit_qlaunch(mylaunch, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding apps to the database. No need to run twice.\n",
    "add_app(name='trainer',\n",
    "        executable='python /gpfs/mira-home/keceli/ffn/keceli_ffn/train_hvd.py',\n",
    "        description='Distributed FFN training script',\n",
    "        envscript='/lus/theta-fs0/projects/connectomics_aesp/balsam/training_env.sh')\n",
    "\n",
    "add_app(name='inference',\n",
    "        executable='python /gpfs/mira-home/keceli/ffn/keceli_ffn/run_inference.py',\n",
    "        description='FFN inference script',\n",
    "        envscript='/lus/theta-fs0/projects/connectomics_aesp/balsam/training_env.sh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a training job to the database\n",
    "import time\n",
    "TFRECORDFILE='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/tf_record_file'\n",
    "GROUNDTRUTH='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/groundtruth.h5'\n",
    "GRAYSCALE='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/grayscale_maps.h5'\n",
    "BATCHSIZE=1\n",
    "OPTIMIZER='adam'\n",
    "TIMESTAMP=time.strftime(\"%y%m%d%H%M%S\")\n",
    "TRAINDIR=f'train_b{BATCHSIZE}_o{OPTIMIZER}_{TIMESTAMP}'\n",
    "\n",
    "myargs = ''\n",
    "myargs += f' --train_coords {TFRECORDFILE} '\n",
    "myargs += f' --data_volumes valdation1:{GRAYSCALE}:raw '\n",
    "myargs += f' --label_volumes valdation1:{GROUNDTRUTH}:stack '\n",
    "myargs += f' --model_name convstack_3d.ConvStack3DFFNModel '\n",
    "myargs += ''' --model_args \"{\\\\\"depth\\\\\": 12, \\\\\"fov_size\\\\\": [33, 33, 33], \\\\\"deltas\\\\\": [8, 8, 8]}\"'''\n",
    "myargs += ' --image_mean 128 --image_stddev 33 '\n",
    "myargs += ' --max_steps 400 --summary_rate_secs 60 ' \n",
    "myargs += f' --batch_size {BATCHSIZE} '\n",
    "myargs += f' --optimizer {OPTIMIZER} '\n",
    "myargs += ' --num_intra_threads 64 --num_inter_threads 1 '\n",
    "myargs += f' --train_dir {TRAINDIR} '\n",
    "print(myargs)\n",
    "\n",
    "add_job(name='test_train',workflow='ffn_training',application='trainer',args=myargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate subvolumes for inference \n",
    "import sys\n",
    "sys.path.insert(0,'/gpfs/mira-home/keceli/ffn/keceli_ffn/')\n",
    "sys.path.insert(0,'/lus/theta-fs0/projects/connectomics_aesp/keceli/pip_ffn/')\n",
    "sys.path.insert(0,'/soft/datascience/tensorflow/tf1.13/')\n",
    "\n",
    "from ffn.utils import bounding_box\n",
    "from ffn.utils import geom_utils\n",
    "bbox=bounding_box.BoundingBox(start=[0,0,0],size=[250,250,250])\n",
    "def divide_bounding_box(bbox, subvolume_size, overlap):\n",
    "    \"\"\"\n",
    "    Returns a list of bounding boxes that divides the given bounding box into subvolumes.\n",
    "    Parameters\n",
    "    ----------\n",
    "    bbox: BoundingBox object,\n",
    "    subvolume_size: list or tuple\n",
    "    overlap: list or tuple\n",
    "    \"\"\"\n",
    "    start = geom_utils.ToNumpy3Vector(bbox.start)\n",
    "    size = geom_utils.ToNumpy3Vector(bbox.size)\n",
    "    bbox = bounding_box.BoundingBox(start, size)\n",
    "    calc = bounding_box.OrderlyOverlappingCalculator(outer_box=bbox, \n",
    "                                                    sub_box_size=subvolume_size, \n",
    "                                                    overlap=overlap, \n",
    "                                                    include_small_sub_boxes=True,\n",
    "                                                    back_shift_small_sub_boxes=False)\n",
    "    return [bb for bb in calc.generate_sub_boxes()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subvolumes: 27\n"
     ]
    }
   ],
   "source": [
    "# Add inference jobs for each subvolume\n",
    "boxes = divide_bounding_box(bbox,subvolume_size=(100,100,100),overlap=(10,10,10))\n",
    "print(f'Number of subvolumes: {len(boxes)}')\n",
    "for i,box in enumerate(boxes):\n",
    "    start = box.start\n",
    "    size  = box.size\n",
    "    inference_args  = ''' --inference_request=\"$(cat /lus/theta-fs0/projects/connectomics_aesp/keceli/inference/config.txt)\" '''\n",
    "    inference_args += f\" --bounding_box 'start {{ x:{start[0]} y:{start[1]} z:{start[2]} }} size {{ x:{size[0]} y:{size[1]} z:{size[2]} }}' \"\n",
    "    add_job(name=f'debug_inference_{i}',\n",
    "            workflow='debug_inference',\n",
    "            application='inference',\n",
    "            args=inference_args,\n",
    "            environ_vars='OMP_NUM_THREADS=64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submit OK: Qlaunch {   'command': '/lus/theta-fs0/projects/connectomics_aesp/balsam_database/qsubmit/qlaunch18.sh',\n",
      "    'from_balsam': True,\n",
      "    'id': 18,\n",
      "    'job_mode': 'serial',\n",
      "    'nodes': 4,\n",
      "    'prescheduled_only': False,\n",
      "    'project': 'connectomics_aesp',\n",
      "    'queue': 'debug-cache-quad',\n",
      "    'scheduler_id': 354724,\n",
      "    'state': 'submitted',\n",
      "    'wall_minutes': 59,\n",
      "    'wf_filter': 'debug_inference'}\n"
     ]
    }
   ],
   "source": [
    "# If you see 'Submit OK:', Job submission is succesful.\n",
    "submit(project='connectomics_aesp',\n",
    "       job_mode='serial',\n",
    "       queue='debug-cache-quad',\n",
    "       nodes=4,\n",
    "       wall_minutes=59,\n",
    "       wf_filter='debug_inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submit OK: Qlaunch {   'command': '/lus/theta-fs0/projects/connectomics_aesp/balsam_database/qsubmit/qlaunch19.sh',\n",
      "    'from_balsam': True,\n",
      "    'id': 19,\n",
      "    'job_mode': 'mpi',\n",
      "    'nodes': 3,\n",
      "    'prescheduled_only': False,\n",
      "    'project': 'connectomics_aesp',\n",
      "    'queue': 'debug-flat-quad',\n",
      "    'scheduler_id': 354725,\n",
      "    'state': 'submitted',\n",
      "    'wall_minutes': 59,\n",
      "    'wf_filter': 'ffn_sub_inference'}\n"
     ]
    }
   ],
   "source": [
    "# If you see 'Submit OK:', Job submission is succesful.\n",
    "submit(project='connectomics_aesp',\n",
    "       queue='debug-flat-quad',\n",
    "       nodes=3,\n",
    "       wall_minutes=59,\n",
    "       wf_filter='ffn_sub_inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/lus/theta-fs0/projects/datascience/jupyterhub\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
